{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577cbfa1-d8ed-4f86-ab2a-511ff745deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightrdf\n",
    "\n",
    "# The folder paths, can be changed as necessary\n",
    "HOME_FOLDER_PATH = \"home/\"  # Expected to house the data, input and output folders (shown just below)\n",
    "DATA_FOLDER_PATH = \"data/\"  # The folder with the KG to be validated\n",
    "INPUT_FOLDER_PATH = \"input_output/\"      # The folder housing the input files necessary for the KG's validation through external information\n",
    "OUTPUT_FOLDER_PATH = \"input_output/\"     # The folder where this Jupiter Notebook's output is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cc528f5-c773-4772-be8f-bf8a70cee730",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_file_name = \"yago-4.5.0.2-tiny/yago-tiny.ttl\"  # The file name or file path of the KG to be validated\n",
    "data_file_path = HOME_FOLDER_PATH + DATA_FOLDER_PATH + kg_file_name\n",
    "\n",
    "doc = lightrdf.RDFDocument(data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3302428-85ed-465a-a75c-df4c455ef636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<http://yago-knowledge.org/schema#>', '<http://www.w3.org/2000/01/rdf-schema#label>', '\"Manual YAGO 4.5 shapes declaration\"@en')\n",
      "('<http://schema.org/CreativeWork>', '<http://www.w3.org/2000/01/rdf-schema#subClassOf>', '<http://schema.org/Thing>')\n",
      "('<http://schema.org/CreativeWork>', '<http://www.w3.org/ns/shacl#property>', '<http://yago-knowledge.org/schema#CreativeWork_property_27>')\n",
      "('<http://schema.org/CreativeWork>', '<http://www.w3.org/ns/shacl#property>', '<http://yago-knowledge.org/schema#CreativeWork_property_38>')\n",
      "('<http://schema.org/CreativeWork>', '<http://www.w3.org/ns/shacl#property>', '<http://yago-knowledge.org/schema#CreativeWork_property_20>')\n",
      "('<http://schema.org/CreativeWork>', '<http://www.w3.org/ns/shacl#property>', '<http://yago-knowledge.org/schema#CreativeWork_property_28>')\n",
      "('<http://schema.org/CreativeWork>', '<http://www.w3.org/ns/shacl#property>', '<http://yago-knowledge.org/schema#CreativeWork_property_19>')\n",
      "('<http://schema.org/CreativeWork>', '<http://www.w3.org/ns/shacl#property>', '<http://yago-knowledge.org/schema#CreativeWork_property_21>')\n",
      "('<http://schema.org/CreativeWork>', '<http://yago-knowledge.org/schema#fromClass>', '<http://www.wikidata.org/entity/Q17537576>')\n",
      "('<http://schema.org/CreativeWork>', '<http://yago-knowledge.org/schema#fromClass>', '<http://www.wikidata.org/entity/Q386724>')\n",
      "('<http://schema.org/CreativeWork>', '<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>', '<http://www.w3.org/ns/shacl#NodeShape>')\n",
      "('<http://schema.org/CreativeWork>', '<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>', '<http://www.w3.org/2000/01/rdf-schema#Class>')\n",
      "('<http://schema.org/Event>', '<http://www.w3.org/2000/01/rdf-schema#subClassOf>', '<http://schema.org/Thing>')\n",
      "('<http://schema.org/Event>', '<http://www.w3.org/ns/shacl#property>', '<http://yago-knowledge.org/schema#Event_property_107>')\n",
      "('<http://schema.org/Event>', '<http://www.w3.org/ns/shacl#property>', '<http://yago-knowledge.org/schema#Event_property_113>')\n",
      "('<http://schema.org/Event>', '<http://www.w3.org/ns/shacl#property>', '<http://yago-knowledge.org/schema#Event_property_106>')\n",
      "('<http://schema.org/Event>', '<http://www.w3.org/ns/shacl#property>', '<http://yago-knowledge.org/schema#Event_property_105>')\n",
      "('<http://schema.org/Event>', '<http://www.w3.org/ns/shacl#property>', '<http://yago-knowledge.org/schema#Event_property_140>')\n",
      "('<http://schema.org/Event>', '<http://www.w3.org/ns/shacl#property>', '<http://yago-knowledge.org/schema#Event_property_133>')\n"
     ]
    }
   ],
   "source": [
    "# Extra Cell 1 (No need to run it, if running the main experiments is the primary intension)\n",
    "parser = lightrdf.Parser()\n",
    "\n",
    "# Can be used to print the first triple's up to the number specified in \"line_num\"\n",
    "line_num = 1\n",
    "for triple in parser.parse(data_file_path, base_iri=None):\n",
    "    print(triple)\n",
    "    line_num+=1\n",
    "    if line_num == 20:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a991b2-54d5-4ea9-aa90-cf1deac072f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23260619\n",
      "12454422\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "# Extra Cell 2 (No need to run it, if running the main experiments is the primary intension)\n",
    "# Contains counts for all triples, the unique entities and the unique relationships.\n",
    "# Given its memory intensive nature, there is no need to rerun, once the results have been created.\n",
    "parser = lightrdf.Parser()\n",
    "\n",
    "line_num = 0\n",
    "entity_set = set()\n",
    "relation_set = set()\n",
    "for triple in doc.search_triples(None, None, None):\n",
    "    #print(triple)\n",
    "    line_num+=1\n",
    "    #print(line_num)\n",
    "    entity_set.add(triple[0])\n",
    "    relation_set.add(triple[1])\n",
    "    entity_set.add(triple[2])\n",
    "\n",
    "print(line_num) # The total amount of triples found\n",
    "print(len(list(entity_set)))  # The number of unique entities found\n",
    "print(len(list(relation_set)))  # The number of unique relationships found\n",
    "# The next two are needed to free up the memory taken by the sets, so that the rest of the experiment can be run freely.\n",
    "entity_set = set()\n",
    "relation_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b89939-1408-4189-971f-60f566379d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal flow continues from here\n",
    "from lightrdf import Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246b4d26-e3fd-40d0-84e9-242a12a4808b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:25: SyntaxWarning: invalid escape sequence '\\^'\n",
      "<>:25: SyntaxWarning: invalid escape sequence '\\^'\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_7596\\3903998319.py:25: SyntaxWarning: invalid escape sequence '\\^'\n",
      "  triple_2_re = re.sub(\"\\^\\^\\<http:\\/\\/www\\.w3\\.org\\/2001\\/XMLSchema\\#dateTime\\>\", \"\", triple[2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300556\n",
      "228774\n",
      "1000-01-01 00:00:00+00:00\n",
      "('<http://yago-knowledge.org/resource/Augusto_Pinochet>', [['<http://schema.org/birthDate>', datetime.datetime(1915, 11, 25, 0, 0, tzinfo=datetime.timezone.utc)], ['<http://schema.org/deathDate>', datetime.datetime(2006, 12, 10, 0, 0, tzinfo=datetime.timezone.utc)]])\n",
      "('<http://yago-knowledge.org/resource/Andrei_Tarkovsky>', [['<http://schema.org/birthDate>', datetime.datetime(1932, 4, 4, 0, 0, tzinfo=datetime.timezone.utc)], ['<http://schema.org/deathDate>', datetime.datetime(1986, 12, 29, 0, 0, tzinfo=datetime.timezone.utc)]])\n",
      "('<http://yago-knowledge.org/resource/Angola>', [['<http://schema.org/dateCreated>', datetime.datetime(1992, 8, 25, 0, 0, tzinfo=datetime.timezone.utc)]])\n",
      "('<http://yago-knowledge.org/resource/Andrei_Sakharov>', [['<http://schema.org/birthDate>', datetime.datetime(1921, 5, 21, 0, 0, tzinfo=datetime.timezone.utc)], ['<http://schema.org/deathDate>', datetime.datetime(1989, 12, 14, 0, 0, tzinfo=datetime.timezone.utc)]])\n",
      "('<http://yago-knowledge.org/resource/Antipolo>', [['<http://schema.org/dateCreated>', datetime.datetime(1650, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)]])\n",
      "('<http://yago-knowledge.org/resource/Abel_Mutai>', [['<http://schema.org/birthDate>', datetime.datetime(1988, 10, 2, 0, 0, tzinfo=datetime.timezone.utc)]])\n",
      "('<http://yago-knowledge.org/resource/Ashton_Eaton>', [['<http://schema.org/birthDate>', datetime.datetime(1988, 1, 21, 0, 0, tzinfo=datetime.timezone.utc)]])\n",
      "('<http://yago-knowledge.org/resource/ACF_Fiorentina>', [['<http://schema.org/dateCreated>', datetime.datetime(1926, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)]])\n",
      "('<http://yago-knowledge.org/resource/Arica_y_Parinacota_Region>', [['<http://schema.org/dateCreated>', datetime.datetime(2007, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)]])\n",
      "('<http://yago-knowledge.org/resource/A_u002E_C_u002E_R_u002E__Siena_1904>', [['<http://schema.org/dateCreated>', datetime.datetime(1904, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)]])\n",
      "-------------\n",
      "6\n",
      "('<http://schema.org/birthDate>', 157180)\n",
      "('<http://schema.org/deathDate>', 69758)\n",
      "('<http://schema.org/dateCreated>', 67929)\n",
      "('<http://schema.org/startDate>', 1469)\n",
      "('<http://schema.org/dissolutionDate>', 3132)\n",
      "('<http://schema.org/endDate>', 1088)\n"
     ]
    }
   ],
   "source": [
    "# Find the temporal triples. in YAGO Tiny, these triples are those with o from <s,p,o> a dateTime object.\n",
    "# The minimum date is also found and, if desirable, removed from all the dates.\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "doc = lightrdf.RDFDocument(data_file_path)\n",
    "\n",
    "regex_var =  Regex(\"\\\"[1-9][0-9]{3}-.+T[^.]+(Z|[+-].+)\\\"\")\n",
    "\n",
    "# In this dict, the s is the key while a list is the value, with this list containing the sublists with [p, o] temporal values\n",
    "temporal_kg_data_dict = {}\n",
    "temporal_categs_dict = {}\n",
    "\n",
    "destination_file_name = \"temporal_triples_file.txt\"  # The name of the file to be created housing the temporal triples.\n",
    "\n",
    "temporal_triple_export_file_name = HOME_FOLDER_PATH + OUTPUT_FOLDER_PATH + destination_file_name\n",
    "temporal_triple_strings_list = []\n",
    "\n",
    "line_num = 0\n",
    "min_date = None\n",
    "# The below regex searches for \"date\"-related objects.\n",
    "for triple in doc.search_triples(None, None, regex_var):\n",
    "    # The following lines aim to leave only the date itself from the \"dateTime\" part of the tail entity.\n",
    "    # Since YAGO Tiny's temporal triples are all of \"dateTime\" form, remove the XMLSchema\\dateTime identifier and all the \"\\\" that remain\n",
    "    triple_2_re = re.sub(\"\\^\\^\\<http:\\/\\/www\\.w3\\.org\\/2001\\/XMLSchema\\#dateTime\\>\", \"\", triple[2])\n",
    "    #print(triple_2_re)\n",
    "    triple_2_nd = re.sub(\"\\\"\", \"\", triple_2_re)\n",
    "    #print(triple_2_nd)\n",
    "    # Determine if there is additional text remaining, because if there is, then the triple is not temporal\n",
    "    triple_2_text = re.findall(\"[a-z]\", triple_2_nd)\n",
    "    \n",
    "    # Proceed only if the tail entity, after the above process, is indeed only a date, meaning the triple is temporal\n",
    "    if len(triple_2_text) == 0:\n",
    "        # Append the triple to the list of temporal triples, so that it can be printed to a file afterwards\n",
    "        temporal_triple_strings_list.append(str(triple[0])+\"\\t\"+str(triple[1])+\"\\t\"+str(triple[2]))\n",
    "        # Transform the date to an understandable format and also update the record of which the minimum date is, if a new minimum one is found\n",
    "        triple_2 = datetime.fromisoformat(triple_2_nd)\n",
    "        if min_date is not None:\n",
    "            if triple_2 < min_date:\n",
    "                min_date = triple_2\n",
    "        else:\n",
    "            min_date = triple_2\n",
    "        #print(triple_2)\n",
    "        # Add the temporal triple to the dict, s as the key, [p, o] as the value\n",
    "        if triple[0] not in temporal_kg_data_dict:\n",
    "            temporal_kg_data_dict[triple[0]] = []\n",
    "        temporal_kg_data_dict[triple[0]].append([triple[1], triple_2])\n",
    "        if triple[1] not in temporal_categs_dict:\n",
    "            temporal_categs_dict[triple[1]] = 0\n",
    "        temporal_categs_dict[triple[1]] += 1\n",
    "    line_num += 1  # This also counts the triples that ultimately were not temporal after all, due to containing text in their tail entity\n",
    "    \n",
    "\n",
    "#print(temporal_kg_data_dict.items())\n",
    "print(len(temporal_triple_strings_list))\n",
    "dict_list = list(temporal_kg_data_dict.items())\n",
    "print(len(dict_list))\n",
    "\n",
    "print(min_date)\n",
    "\"\"\"\n",
    "# Removing of the minimun date from all the records.\n",
    "# Not necessary, but was implemented at some earlier point.\n",
    "for dict_idx in range(len(dict_list)):\n",
    "    for ins_idx in range(len(dict_list[dict_idx][1])):\n",
    "        #dict_list[dict_idx][1][ins_idx][1] = datetime(dict_list[dict_idx][1][ins_idx][1] - min_date)\n",
    "        dict_list[dict_idx][1][ins_idx][1] = dict_list[dict_idx][1][ins_idx][1] - min_date  # the result is a timedelta itself\n",
    "        # The following line results in the AttributeError: 'datetime.timedelta' object has no attribute 'strftime'\n",
    "        #dict_list[dict_idx][1][ins_idx][1] = (dict_list[dict_idx][1][ins_idx][1] - min_date).strftime(\"%Y%m%d\")\n",
    "        \n",
    "        #timedelta_temp_result = dict_list[dict_idx][1][ins_idx][1] - min_date\n",
    "        #timedelta_temp_result = timedelta(dict_list[dict_idx][1][ins_idx][1] - min_date)  # Results in an error message\n",
    "        #dict_list[dict_idx][1][ins_idx][1] = dict_list[dict_idx][1][ins_idx][1] - timedelta_temp_result\n",
    "\"\"\"\n",
    "# List the first 10 temporal triples\n",
    "line_num = 0\n",
    "for dict_idx in range(len(dict_list)):\n",
    "    print(dict_list[dict_idx])\n",
    "    line_num += 1\n",
    "    if line_num >= 10:\n",
    "        break;\n",
    "\n",
    "print(\"-------------\")\n",
    "# List all the temporal triples found, aggregated according to their predicate/relation\n",
    "dict_list = list(temporal_categs_dict.items())\n",
    "print(len(dict_list))\n",
    "for dict_idx in range(len(dict_list)):\n",
    "    print(dict_list[dict_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16b89b77-ce04-49f1-bda0-ff5472bc3d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228774\n",
      "0 <http://schema.org/birthDate>\t<\t<http://schema.org/deathDate> 46685 0.650372\n",
      "1 <http://schema.org/deathDate>\t>\t<http://schema.org/birthDate> 20876 0.290825\n",
      "2 <http://schema.org/dateCreated>\t==\t<http://schema.org/dateCreated> 215 0.002995\n",
      "3 <http://schema.org/dateCreated>\t<\t<http://schema.org/dissolutionDate> 2357 0.032836\n",
      "4 <http://schema.org/dissolutionDate>\t>\t<http://schema.org/dateCreated> 427 0.005949\n",
      "5 <http://schema.org/birthDate>\t>\t<http://schema.org/deathDate> 42 0.000585\n",
      "6 <http://schema.org/deathDate>\t<\t<http://schema.org/birthDate> 46 0.000641\n",
      "7 <http://schema.org/dateCreated>\t==\t<http://schema.org/dissolutionDate> 31 0.000432\n",
      "8 <http://schema.org/endDate>\t>\t<http://schema.org/startDate> 17 0.000237\n",
      "9 <http://schema.org/startDate>\t<\t<http://schema.org/endDate> 965 0.013443\n",
      "10 <http://schema.org/birthDate>\t==\t<http://schema.org/deathDate> 42 0.000585\n",
      "11 <http://schema.org/dateCreated>\t>\t<http://schema.org/dissolutionDate> 3 4.2e-05\n",
      "12 <http://schema.org/startDate>\t==\t<http://schema.org/endDate> 58 0.000808\n",
      "13 <http://schema.org/dissolutionDate>\t<\t<http://schema.org/dateCreated> 2 2.8e-05\n",
      "14 <http://schema.org/deathDate>\t==\t<http://schema.org/birthDate> 14 0.000195\n",
      "15 <http://schema.org/startDate>\t>\t<http://schema.org/endDate> 1 1.4e-05\n",
      "16 <http://schema.org/dissolutionDate>\t==\t<http://schema.org/dateCreated> 1 1.4e-05\n",
      "['<http://yago-knowledge.org/resource/Augusto_Pinochet>', 0]\n",
      "['<http://yago-knowledge.org/resource/Andrei_Tarkovsky>', 0]\n",
      "['<http://yago-knowledge.org/resource/Andrei_Sakharov>', 0]\n",
      "['<http://yago-knowledge.org/resource/Alberich_of_Reims>', 1]\n",
      "['<http://yago-knowledge.org/resource/Anthony_Sweijs>', 0]\n",
      "['<http://yago-knowledge.org/resource/Ars_Magica>', 2]\n",
      "['<http://yago-knowledge.org/resource/Andrea_del_Sarto>', 1]\n",
      "['<http://yago-knowledge.org/resource/Albrecht_Dürer>', 0]\n",
      "['<http://yago-knowledge.org/resource/Antonello_da_Messina>', 0]\n",
      "['<http://yago-knowledge.org/resource/Agnes_Pockels>', 0]\n",
      "-----\n",
      "{'<http://schema.org/birthDate>\\t<http://schema.org/deathDate>': {'<': 46685, '>': 42, '==': 42}, '<http://schema.org/deathDate>\\t<http://schema.org/birthDate>': {'>': 20876, '<': 46, '==': 14}, '<http://schema.org/dateCreated>\\t<http://schema.org/dateCreated>': {'==': 215}, '<http://schema.org/dateCreated>\\t<http://schema.org/dissolutionDate>': {'<': 2357, '==': 31, '>': 3}, '<http://schema.org/dissolutionDate>\\t<http://schema.org/dateCreated>': {'>': 427, '<': 2, '==': 1}, '<http://schema.org/endDate>\\t<http://schema.org/startDate>': {'>': 17}, '<http://schema.org/startDate>\\t<http://schema.org/endDate>': {'<': 965, '==': 58, '>': 1}}\n"
     ]
    }
   ],
   "source": [
    "# Finds the temporal relations and their different variants.\n",
    "# Also remembers the order of appearance of the different temporal relation variants, for eventual usage in frequent pattern mining.\n",
    "# Does not have duplicate relations, but cases like DateTypeA < DateTypeB and DataTypeB > DataTypeA are not considered the same.\n",
    "dict_list = list(temporal_kg_data_dict.items())\n",
    "print(len(dict_list))\n",
    "\n",
    "# Stores the name of the temporal relation as \"temporal_predicate_1\\t(in)equality\\ttemporal_predicate_2\"\n",
    "temporal_relation_names_list = []\n",
    "# Stores how many times each element from the \"temporal_relation_names_list\" just above has appeared\n",
    "temporal_relation_counters = {}\n",
    "# A list containing the subject as its first element, and as the rest the indexes of the temporal relations triples having this subject satisfy.\n",
    "# The indexes refer to the temporal relations as they appear in \"temporal_relation_names_list\", in order of appearance.\n",
    "# This could be useful for extensions with regards to association rule mining.\n",
    "s_and_temporal_true_index_list = []\n",
    "# The dict the counters of the temporal relations in the form \"temporal_predicate_1\\ttemporal_predicate_2\"\"(in)equality\":counter_number\n",
    "temporal_key_value_relations_counters = {}\n",
    "\n",
    "for dict_idx in range(len(dict_list)):\n",
    "    # Continue only if the current subject is a subject two or more <s,p,o> temporal triples, according to what has been found above\n",
    "    if len(dict_list[dict_idx][1]) > 1:\n",
    "        # The list contains the current subject and the indexes of the temporal relations found, as expressed in the \"temporal_relation_names_list\"\n",
    "        current_s_and_temporal_list = [dict_list[dict_idx][0]]  # Initialization of said list with the subject\n",
    "        #print(current_s_and_temporal_list)\n",
    "        # Run through all the potential pairs of [p, o] of the current s\n",
    "        for trgt_idx in range(len(dict_list[dict_idx][1])):\n",
    "            for othr_idx in range(len(dict_list[dict_idx][1])-1, -1, -1):\n",
    "                if trgt_idx < othr_idx:\n",
    "                    # This case refers to when DateTypeA's value is smaller (<) than DateTypeB's value\n",
    "                    if dict_list[dict_idx][1][trgt_idx][1] < dict_list[dict_idx][1][othr_idx][1]:\n",
    "                        #print(str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t<\\t\"+dict_list[dict_idx][1][othr_idx][0]))  # This should work as a key\n",
    "                        # If the temporal relationship of \"DateTypeA < DateTypeB\" does not exist in the \"temporal_relation_names_list\", add it\n",
    "                        if str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t<\\t\"+dict_list[dict_idx][1][othr_idx][0]) not in temporal_relation_names_list:\n",
    "                            temporal_relation_names_list.append(str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t<\\t\"+dict_list[dict_idx][1][othr_idx][0]))\n",
    "                            temporal_relation_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t<\\t\"+dict_list[dict_idx][1][othr_idx][0])] = 0\n",
    "                            if str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t\"+dict_list[dict_idx][1][othr_idx][0]) not in temporal_key_value_relations_counters.keys():\n",
    "                                temporal_key_value_relations_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t\"+dict_list[dict_idx][1][othr_idx][0])] = {}\n",
    "                            temporal_key_value_relations_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t\"+dict_list[dict_idx][1][othr_idx][0])][\"<\"] = 0\n",
    "                        #print(temporal_relation_names_list.index(str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t<\\t\"+dict_list[dict_idx][1][othr_idx][0])))\n",
    "                        # Append the index of the temporal relationship of \"temporal_relation_names_list\" found in this case to be True\n",
    "                        current_s_and_temporal_list.append(temporal_relation_names_list.index(str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t<\\t\"+dict_list[dict_idx][1][othr_idx][0])))\n",
    "                        temporal_relation_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t<\\t\"+dict_list[dict_idx][1][othr_idx][0])] += 1\n",
    "                        temporal_key_value_relations_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t\"+dict_list[dict_idx][1][othr_idx][0])][\"<\"] += 1\n",
    "                    # This case refers to when DateTypeA's value is larger (>) than DateTypeB's value\n",
    "                    elif dict_list[dict_idx][1][trgt_idx][1] > dict_list[dict_idx][1][othr_idx][1]:\n",
    "                        #print(str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t>\\t\"+dict_list[dict_idx][1][othr_idx][0]))  # This should work as a key\n",
    "                        # If the temporal relationship of \"DateTypeA > DateTypeB\" does not exist in the \"temporal_relation_names_list\", add it\n",
    "                        if str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t>\\t\"+dict_list[dict_idx][1][othr_idx][0]) not in temporal_relation_names_list:\n",
    "                            temporal_relation_names_list.append(str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t>\\t\"+dict_list[dict_idx][1][othr_idx][0]))\n",
    "                            temporal_relation_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t>\\t\"+dict_list[dict_idx][1][othr_idx][0])] = 0\n",
    "                            if str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t\"+dict_list[dict_idx][1][othr_idx][0]) not in temporal_key_value_relations_counters.keys():\n",
    "                                temporal_key_value_relations_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t\"+dict_list[dict_idx][1][othr_idx][0])] = {}\n",
    "                            temporal_key_value_relations_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t\"+dict_list[dict_idx][1][othr_idx][0])][\">\"] = 0\n",
    "                        #print(temporal_relation_names_list.index(str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t>\\t\"+dict_list[dict_idx][1][othr_idx][0])))\n",
    "                        # Append the index of the temporal relationship of \"temporal_relation_names_list\" found in this case to be True\n",
    "                        current_s_and_temporal_list.append(temporal_relation_names_list.index(str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t>\\t\"+dict_list[dict_idx][1][othr_idx][0])))\n",
    "                        temporal_relation_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t>\\t\"+dict_list[dict_idx][1][othr_idx][0])] += 1\n",
    "                        temporal_key_value_relations_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t\"+dict_list[dict_idx][1][othr_idx][0])][\">\"] += 1\n",
    "                    # This case refers to when DateTypeA's value is equal (==) to DateTypeB's value\n",
    "                    elif dict_list[dict_idx][1][trgt_idx][1] == dict_list[dict_idx][1][othr_idx][1]:\n",
    "                        #print(str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t==\\t\"+dict_list[dict_idx][1][othr_idx][0]))  # This should work as a key\n",
    "                        # If the temporal relationship of \"DateTypeA == DateTypeB\" does not exist in the \"temporal_relation_names_list\", add it\n",
    "                        if str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t==\\t\"+dict_list[dict_idx][1][othr_idx][0]) not in temporal_relation_names_list:\n",
    "                            temporal_relation_names_list.append(str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t==\\t\"+dict_list[dict_idx][1][othr_idx][0]))\n",
    "                            temporal_relation_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t==\\t\"+dict_list[dict_idx][1][othr_idx][0])] = 0\n",
    "                            if str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t\"+dict_list[dict_idx][1][othr_idx][0]) not in temporal_key_value_relations_counters.keys():\n",
    "                                temporal_key_value_relations_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t\"+dict_list[dict_idx][1][othr_idx][0])] = {}\n",
    "                            temporal_key_value_relations_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t\"+dict_list[dict_idx][1][othr_idx][0])][\"==\"] = 0\n",
    "                        #print(temporal_relation_names_list.index(str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t==\\t\"+dict_list[dict_idx][1][othr_idx][0])))\n",
    "                        # Append the index of the temporal relationship of \"temporal_relation_names_list\" found in this case to be True\n",
    "                        current_s_and_temporal_list.append(temporal_relation_names_list.index(str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t==\\t\"+dict_list[dict_idx][1][othr_idx][0])))\n",
    "                        temporal_relation_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t==\\t\"+dict_list[dict_idx][1][othr_idx][0])] += 1\n",
    "                        temporal_key_value_relations_counters[str(dict_list[dict_idx][1][trgt_idx][0]+\"\\t\"+dict_list[dict_idx][1][othr_idx][0])][\"==\"] += 1\n",
    "        if len(current_s_and_temporal_list) > 1:\n",
    "            # Append the list of the current subject and its temporal relationship indexes to the one containing all such lists only if\n",
    "            # at least one of the temporal relationships from the \"temporal_relation_names_list\" is found, otherwise no such need exists.\n",
    "            s_and_temporal_true_index_list.append(current_s_and_temporal_list)\n",
    "\n",
    "# Counts the amount of temporal relations found, from the subjects that have at least two temporal [p, o] pairs\n",
    "total_temporal_relations = sum(temporal_relation_counters.values())\n",
    "\n",
    "# Shows the index of each temporal relation, the temporal relation, the times it appeared and\n",
    "# the support of the relation, compared to all the temporal relations found.\n",
    "for item_idx in range(len(temporal_relation_names_list)):\n",
    "    print(item_idx, temporal_relation_names_list[item_idx], temporal_relation_counters[temporal_relation_names_list[item_idx]], round((float(temporal_relation_counters[temporal_relation_names_list[item_idx]])/total_temporal_relations), 6))\n",
    "\n",
    "for item_idx in range(10):\n",
    "    print(s_and_temporal_true_index_list[item_idx])\n",
    "\n",
    "print(\"-----\")\n",
    "\n",
    "print(temporal_key_value_relations_counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aae25568-6075-4749-a117-cdbcd5bd42b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<http://schema.org/birthDate>\\t<http://schema.org/deathDate>': {'<': 46685, '>': 42, '==': 42}, '<http://schema.org/deathDate>\\t<http://schema.org/birthDate>': {'>': 20876, '<': 46, '==': 14}, '<http://schema.org/dateCreated>\\t<http://schema.org/dateCreated>': {'==': 215}, '<http://schema.org/dateCreated>\\t<http://schema.org/dissolutionDate>': {'<': 2357, '==': 31, '>': 3}, '<http://schema.org/dissolutionDate>\\t<http://schema.org/dateCreated>': {'>': 427, '<': 2, '==': 1}, '<http://schema.org/endDate>\\t<http://schema.org/startDate>': {'>': 17}, '<http://schema.org/startDate>\\t<http://schema.org/endDate>': {'<': 965, '==': 58, '>': 1}}\n",
      "[['<http://schema.org/birthDate>', '<http://schema.org/deathDate>'], ['<http://schema.org/deathDate>', '<http://schema.org/birthDate>'], ['<http://schema.org/dateCreated>', '<http://schema.org/dateCreated>'], ['<http://schema.org/dateCreated>', '<http://schema.org/dissolutionDate>'], ['<http://schema.org/dissolutionDate>', '<http://schema.org/dateCreated>'], ['<http://schema.org/endDate>', '<http://schema.org/startDate>'], ['<http://schema.org/startDate>', '<http://schema.org/endDate>']]\n"
     ]
    }
   ],
   "source": [
    "print(temporal_key_value_relations_counters)\n",
    "# Stores all the possible interval cases encountered before, as [\"temporal_predicate_1\", \"temporal_predicate_2\"].\n",
    "# Essentially, splits some of the previously found information in its tab (\\t) character and stores the two predicates in the list.\n",
    "interval_cases_list = []\n",
    "line_num = 0\n",
    "for line_num in range(len(list(temporal_key_value_relations_counters.keys()))):\n",
    "    #print(list(temporal_key_value_relations_counters.keys())[line_num].split('\\t'))\n",
    "    # Split the two temporal predicates that form the temporal relation by their tab character\n",
    "    # separaring them and store them to the list, in the form shown in the above comment.\n",
    "    interval_cases_list.append(list(temporal_key_value_relations_counters.keys())[line_num].split('\\t'))\n",
    "    line_num += 1\n",
    "\n",
    "print(interval_cases_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08f1ed6d-f509-4b1d-9c15-260b1ce97ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://yago-knowledge.org/resource/Augusto_Pinochet> {'<http://schema.org/birthDate>\\t<http://schema.org/deathDate>': Interval(1915-11-25 00:00:00, 2006-12-10 00:00:00, closed='both')}\n",
      "<http://yago-knowledge.org/resource/Andrei_Tarkovsky> {'<http://schema.org/birthDate>\\t<http://schema.org/deathDate>': Interval(1932-04-04 00:00:00, 1986-12-29 00:00:00, closed='both')}\n",
      "<http://yago-knowledge.org/resource/Andrei_Sakharov> {'<http://schema.org/birthDate>\\t<http://schema.org/deathDate>': Interval(1921-05-21 00:00:00, 1989-12-14 00:00:00, closed='both')}\n",
      "<http://yago-knowledge.org/resource/Alberich_of_Reims> {'<http://schema.org/birthDate>\\t<http://schema.org/deathDate>': Interval(1085-01-01 00:00:00, 1141-01-01 00:00:00, closed='both')}\n",
      "<http://yago-knowledge.org/resource/Anthony_Sweijs> {'<http://schema.org/birthDate>\\t<http://schema.org/deathDate>': Interval(1852-07-18 00:00:00, 1937-09-30 00:00:00, closed='both')}\n",
      "<http://yago-knowledge.org/resource/Andrea_del_Sarto> {'<http://schema.org/birthDate>\\t<http://schema.org/deathDate>': Interval(1486-07-25 00:00:00, 1530-10-09 00:00:00, closed='both')}\n",
      "<http://yago-knowledge.org/resource/Albrecht_Dürer> {'<http://schema.org/birthDate>\\t<http://schema.org/deathDate>': Interval(1471-05-30 00:00:00, 1528-04-16 00:00:00, closed='both')}\n",
      "<http://yago-knowledge.org/resource/Antonello_da_Messina> {'<http://schema.org/birthDate>\\t<http://schema.org/deathDate>': Interval(1430-01-01 00:00:00, 1479-01-01 00:00:00, closed='both')}\n",
      "<http://yago-knowledge.org/resource/Agnes_Pockels> {'<http://schema.org/birthDate>\\t<http://schema.org/deathDate>': Interval(1862-02-14 00:00:00, 1935-11-21 00:00:00, closed='both')}\n",
      "<http://yago-knowledge.org/resource/Ambrogio_Bergognone> {'<http://schema.org/birthDate>\\t<http://schema.org/deathDate>': Interval(1453-01-01 00:00:00, 1522-01-01 00:00:00, closed='both')}\n"
     ]
    }
   ],
   "source": [
    "# Definition of the temporal intervals\n",
    "import pandas as pd\n",
    "\n",
    "# A dictionary where the key is a subject and, as values, there are additional key-value pairs, where\n",
    "# the value key is the kind of relation (e.g. birthdate\\tdeathdeate) and the value value the interval.\n",
    "temporal_intervals_dict = {}\n",
    "\n",
    "# Find all the cases where intervals can be created and define them\n",
    "for key_subject, values_po_list in temporal_kg_data_dict.items():\n",
    "    # The dict that, for the current subject, each of its [p, o] pairs as p: o (key the predicate, object the value)\n",
    "    temporal_po_dict = {}\n",
    "    for po_pair_list in values_po_list:\n",
    "        temporal_po_dict[po_pair_list[0]] = po_pair_list[1]\n",
    "    # Continue only if the current subject has at least two p: o pairs, so that intervals can be created\n",
    "    if len(temporal_po_dict.keys()) > 1:\n",
    "        # For the current subject, stores the k: v pairs, where they keys are the temporal relations and the values the corresponding interval\n",
    "        temporal_intervals_dict[key_subject] = {}\n",
    "        for trgt_idx in range(len(interval_cases_list)):\n",
    "            if set(temporal_po_dict.keys()).intersection(set(interval_cases_list[trgt_idx])) == set(interval_cases_list[trgt_idx]):\n",
    "                #print(interval_cases_list[trgt_idx])\n",
    "                min_idx = list(temporal_po_dict.values()).index(min(list(temporal_po_dict.values())))\n",
    "                max_idx = list(temporal_po_dict.values()).index(max(list(temporal_po_dict.values())))\n",
    "                min_key = list(temporal_po_dict.keys())[min_idx]\n",
    "                max_key = list(temporal_po_dict.keys())[max_idx]\n",
    "                min_value = datetime.strftime(list(temporal_po_dict.values())[min_idx], '%Y-%m-%d %H:%M:%S')\n",
    "                max_value = datetime.strftime(list(temporal_po_dict.values())[max_idx], '%Y-%m-%d %H:%M:%S')\n",
    "                #print(str(min_value), str(max_value))\n",
    "                #print(pd.Timestamp(str(min_value)), pd.Timestamp(str(max_value)))\n",
    "                temporal_intervals_dict[key_subject][str(min_key+\"\\t\"+max_key)] = pd.Interval(pd.Timestamp(min_value), pd.Timestamp(max_value), closed=\"both\")\n",
    "\n",
    "line_num = 0\n",
    "for key_item, values_item in temporal_intervals_dict.items():\n",
    "    print(key_item, values_item)\n",
    "    line_num += 1\n",
    "    if line_num == 10:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ea2c04e-0722-4320-90e8-523e0ddc501b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://schema.org/birthDate>\t<http://schema.org/deathDate> < 46685 0.9982\n",
      "<http://schema.org/birthDate>\t<http://schema.org/deathDate> > 42 0.0009\n",
      "<http://schema.org/birthDate>\t<http://schema.org/deathDate> == 42 0.0009\n",
      "\n",
      "<http://schema.org/deathDate>\t<http://schema.org/birthDate> > 20876 0.9971\n",
      "<http://schema.org/deathDate>\t<http://schema.org/birthDate> < 46 0.0022\n",
      "<http://schema.org/deathDate>\t<http://schema.org/birthDate> == 14 0.0007\n",
      "\n",
      "<http://schema.org/dateCreated>\t<http://schema.org/dateCreated> == 215 1.0\n",
      "\n",
      "<http://schema.org/dateCreated>\t<http://schema.org/dissolutionDate> < 2357 0.9858\n",
      "<http://schema.org/dateCreated>\t<http://schema.org/dissolutionDate> == 31 0.013\n",
      "<http://schema.org/dateCreated>\t<http://schema.org/dissolutionDate> > 3 0.0013\n",
      "\n",
      "<http://schema.org/dissolutionDate>\t<http://schema.org/dateCreated> > 427 0.993\n",
      "<http://schema.org/dissolutionDate>\t<http://schema.org/dateCreated> < 2 0.0047\n",
      "<http://schema.org/dissolutionDate>\t<http://schema.org/dateCreated> == 1 0.0023\n",
      "\n",
      "<http://schema.org/endDate>\t<http://schema.org/startDate> > 17 1.0\n",
      "\n",
      "<http://schema.org/startDate>\t<http://schema.org/endDate> < 965 0.9424\n",
      "<http://schema.org/startDate>\t<http://schema.org/endDate> == 58 0.0566\n",
      "<http://schema.org/startDate>\t<http://schema.org/endDate> > 1 0.001\n",
      "\n",
      "<http://schema.org/birthDate>\t<http://schema.org/deathDate> < 46685 0.9982\n",
      "<http://schema.org/deathDate>\t<http://schema.org/birthDate> > 20876 0.9971\n",
      "<http://schema.org/dateCreated>\t<http://schema.org/dateCreated> == 215 1.0\n",
      "<http://schema.org/dateCreated>\t<http://schema.org/dissolutionDate> < 2357 0.9858\n",
      "<http://schema.org/dissolutionDate>\t<http://schema.org/dateCreated> > 427 0.993\n",
      "<http://schema.org/endDate>\t<http://schema.org/startDate> > 17 1.0\n",
      "<http://schema.org/startDate>\t<http://schema.org/endDate> < 965 0.9424\n"
     ]
    }
   ],
   "source": [
    "# Find and print all the different temporal relations between two items, as well as the corresponding support values\n",
    "for outer_key_item in temporal_key_value_relations_counters.keys():\n",
    "    total_relation_number = sum(temporal_key_value_relations_counters[outer_key_item].values())\n",
    "    for inner_key_item, inner_value_item in temporal_key_value_relations_counters[outer_key_item].items():\n",
    "        print(outer_key_item, inner_key_item, inner_value_item, round(float(inner_value_item)/total_relation_number, 4))\n",
    "    print()\n",
    "\n",
    "# Find and print only the dominant variant of a temporal relation, based on its support\n",
    "for outer_key_item in temporal_key_value_relations_counters.keys():\n",
    "    total_relation_number = sum(temporal_key_value_relations_counters[outer_key_item].values())\n",
    "    max_value_item = max(list(temporal_key_value_relations_counters[outer_key_item].values()))\n",
    "    max_key_item = list(temporal_key_value_relations_counters[outer_key_item].keys())[list(temporal_key_value_relations_counters[outer_key_item].values()).index(max_value_item)]\n",
    "    print(outer_key_item, max_key_item, max_value_item, round(float(max_value_item)/total_relation_number, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "114edaf7-0128-4147-b59b-540db82cd893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acceptable_equality_cases():\n",
    "    # A list of lists where each internal list contains the elements among which temporal equality is acceptable, if encountered together.\n",
    "    equality_cases_list = []\n",
    "    input_file_name = \"acceptable_equality_cases.txt\"\n",
    "    input_file = open(HOME_FOLDER_PATH + INPUT_FOLDER_PATH + input_file_name)\n",
    "    # Each line of the file contains the predicates among where temporal equality is acceptable, split with a tab\n",
    "    line_content = input_file.readline()  # Read the file's first line\n",
    "    # Continue as long as the line read from the file exists (is not completely empty)\n",
    "    while line_content != '':\n",
    "        line_content = re.sub(\"\\n\", \"\", line_content)  # Remove the line change character\n",
    "        line_content = line_content.split('\\t')  # and split the line's contents by tab.\n",
    "        equality_cases_list.append(line_content)\n",
    "        equality_cases_list.append([line_content[1], line_content[0]])\n",
    "        line_content = input_file.readline()  # Read the file's next line\n",
    "    input_file.close()\n",
    "    return equality_cases_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e53cad7-6d7c-4df5-bb52-3c8f7f75d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acceptable_interval_limits():\n",
    "    # A list of lists where each internal list contains the pairs of predicates making an interval,\n",
    "    # as well as the lower and upper limits of the possible values for said interval.\n",
    "    interval_limits_list = []\n",
    "    input_file_name = \"acceptable_interval_rules.txt\"\n",
    "    input_file = open(HOME_FOLDER_PATH + INPUT_FOLDER_PATH + input_file_name)\n",
    "    # Each line of the file contains two predicates, an upper and a lower limit (from which one might be - if it is not applicable), separated by a tab\n",
    "    line_content = input_file.readline()  # Read the file's first line\n",
    "    # Continue as long as the line read from the file exists (is not completely empty)\n",
    "    while line_content != '':\n",
    "        line_content = re.sub(\"\\n\", \"\", line_content)  # Remove the line change character\n",
    "        line_content = line_content.split('\\t')  # and split the line's contents by tab.\n",
    "        interval_limits_list.append(line_content)\n",
    "        line_content = input_file.readline()  # Read the file's next line\n",
    "    input_file.close()\n",
    "    return interval_limits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83dbad0-ceab-4ed1-9f6b-26a096ca476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the next cells, run whichever is desired, each time.\n",
    "# The first cell only applies the fully-automatic internal validation method for finding inconsistent temporal triples,\n",
    "# the second cell has the fully-automatic method, alongside the relaxation of what constitutes the dominant temporal relation, via external information,\n",
    "# while the third combines the two methods of the second cell, along with temporal, validity intervals, defined through extenral information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23b7505d-d7dc-40a9-a150-35c50a2fb079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<http://schema.org/deathDate>\\t==\\t<http://schema.org/birthDate>', '<http://schema.org/dateCreated>\\t==\\t<http://schema.org/dissolutionDate>', '<http://schema.org/dissolutionDate>\\t==\\t<http://schema.org/dateCreated>', '<http://schema.org/startDate>\\t>\\t<http://schema.org/endDate>', '<http://schema.org/birthDate>\\t>\\t<http://schema.org/deathDate>', '<http://schema.org/startDate>\\t==\\t<http://schema.org/endDate>', '<http://schema.org/dissolutionDate>\\t<\\t<http://schema.org/dateCreated>', '<http://schema.org/dateCreated>\\t>\\t<http://schema.org/dissolutionDate>', '<http://schema.org/birthDate>\\t==\\t<http://schema.org/deathDate>', '<http://schema.org/deathDate>\\t<\\t<http://schema.org/birthDate>'}\n",
      "[14, 7, 16, 15, 5, 12, 13, 11, 10, 6]\n"
     ]
    }
   ],
   "source": [
    "# This applies the automatic method for finding temporal inconsistencies within a TKG, which is reliant\n",
    "# exclusively on the KG's internal data. External information for the temporal relations is not used.\n",
    "automatically_produced_erroneous_patterns = set()\n",
    "\n",
    "# Find the erroneous patterns and print to a file all the erroneous triples\n",
    "for outer_key_item in temporal_key_value_relations_counters.keys():\n",
    "    total_relation_number = sum(temporal_key_value_relations_counters[outer_key_item].values())  # Can be removed\n",
    "    # Get the value of the temporal relation item with the maximum value, which is considered the dominant variant\n",
    "    max_value_item = max(list(temporal_key_value_relations_counters[outer_key_item].values()))\n",
    "    # Get the '<', '>' or '==' symbol that corresponds to the variant of the temporal relation with the maximum value\n",
    "    max_key_item = list(temporal_key_value_relations_counters[outer_key_item].keys())[list(temporal_key_value_relations_counters[outer_key_item].values()).index(max_value_item)]\n",
    "    #print(outer_key_item, max_key_item, max_value_item, round(float(max_value_item)/total_relation_number, 4))\n",
    "    for inner_key_item, inner_value_item in temporal_key_value_relations_counters[outer_key_item].items():\n",
    "        if inner_key_item != max_key_item:\n",
    "            outer_key_split = outer_key_item.split('\\t')\n",
    "            automatically_produced_erroneous_patterns.add(str(outer_key_split[0]+\"\\t\"+inner_key_item+\"\\t\"+outer_key_split[1]))\n",
    "\n",
    "print(automatically_produced_erroneous_patterns)\n",
    "\n",
    "automatically_produced_erroneous_patterns = list(automatically_produced_erroneous_patterns)\n",
    "\n",
    "# Stores the indexes of the patterns found to be potentially inconsistent\n",
    "indexes_of_automatically_produced_erroneous_patterns = []\n",
    "\n",
    "for item_idx in range(len(automatically_produced_erroneous_patterns)):\n",
    "    indexes_of_automatically_produced_erroneous_patterns.append(temporal_relation_names_list.index(automatically_produced_erroneous_patterns[item_idx]))\n",
    "\n",
    "print(indexes_of_automatically_produced_erroneous_patterns)\n",
    "\n",
    "# Stores the subjects (with subject referring to s from the <s,p,o> triples), who have at least one pair of inonsistent temporal triples,\n",
    "# which is a pair of triples satisfying a temporal relationship variant found to be inconsistent.\n",
    "automatically_erroneous_subject_list = []\n",
    "for item_idx in range(len(s_and_temporal_true_index_list)):\n",
    "    if len(set(indexes_of_automatically_produced_erroneous_patterns).intersection(set(s_and_temporal_true_index_list[item_idx][1:]))) > 0:\n",
    "        automatically_erroneous_subject_list.append(s_and_temporal_true_index_list[item_idx][0])\n",
    "\n",
    "destination_file_name = \"automatically_inconsistent_triples_file.txt\"\n",
    "automatically_erroneous_triple_file_name = HOME_FOLDER_PATH + OUTPUT_FOLDER_PATH + destination_file_name\n",
    "automatically_erroneous_triple_file_contents = []\n",
    "\n",
    "# Prints to file all the triples sharing the erroneous subject\n",
    "for item_idx in range(len(temporal_triple_strings_list)):\n",
    "    current_subject = temporal_triple_strings_list[item_idx].split('\\t')[0]\n",
    "    #print(temporal_triple_strings_list[item_idx], current_subject)\n",
    "    if current_subject in automatically_erroneous_subject_list:\n",
    "        #print(temporal_triple_strings_list[item_idx], current_subject, automatically_erroneous_subject_list[0])\n",
    "        automatically_erroneous_triple_file_contents.append(temporal_triple_strings_list[item_idx])\n",
    "\n",
    "automatically_erroneous_triples_file = open(automatically_erroneous_triple_file_name, \"w+\", encoding=\"utf8\")\n",
    "for list_idx in range(len(automatically_erroneous_triple_file_contents)):\n",
    "    automatically_erroneous_triples_file.write(automatically_erroneous_triple_file_contents[list_idx]+\"\\n\")\n",
    "automatically_erroneous_triples_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5afd688-b954-4251-b4b5-ff2ac6fde261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<http://schema.org/startDate>', '<http://schema.org/endDate>'], ['<http://schema.org/endDate>', '<http://schema.org/startDate>']]\n",
      "{'<http://schema.org/deathDate>\\t==\\t<http://schema.org/birthDate>', '<http://schema.org/dateCreated>\\t==\\t<http://schema.org/dissolutionDate>', '<http://schema.org/dissolutionDate>\\t==\\t<http://schema.org/dateCreated>', '<http://schema.org/startDate>\\t>\\t<http://schema.org/endDate>', '<http://schema.org/birthDate>\\t>\\t<http://schema.org/deathDate>', '<http://schema.org/dissolutionDate>\\t<\\t<http://schema.org/dateCreated>', '<http://schema.org/dateCreated>\\t>\\t<http://schema.org/dissolutionDate>', '<http://schema.org/birthDate>\\t==\\t<http://schema.org/deathDate>', '<http://schema.org/deathDate>\\t<\\t<http://schema.org/birthDate>'}\n",
      "[14, 7, 16, 15, 5, 13, 11, 10, 6]\n"
     ]
    }
   ],
   "source": [
    "# This firstly applies the automatic method for finding temporal inconsistencies within a TKG, reliant only on\n",
    "# the KG's internal information, and then uses external information for defining which temporal relations should\n",
    "# have the equality as a part of the dominant variant.\n",
    "automatically_produced_erroneous_patterns = set()\n",
    "\n",
    "# The list with the cases where, for temporal relations with an inequality dominant variant, the equality is also acceptable\n",
    "equality_cases_list = acceptable_equality_cases()\n",
    "print(equality_cases_list)\n",
    "\n",
    "# Find the erroneous patterns and print to a file all the erroneous triples\n",
    "for outer_key_item in temporal_key_value_relations_counters.keys():\n",
    "    total_relation_number = sum(temporal_key_value_relations_counters[outer_key_item].values())  # Can be removed\n",
    "    # Get the value of the temporal relation item with the maximum value, which is considered the dominant variant\n",
    "    max_value_item = max(list(temporal_key_value_relations_counters[outer_key_item].values()))\n",
    "    # Get the '<', '>' or '==' symbol that corresponds to the variant of the temporal relation with the maximum value\n",
    "    max_key_item = list(temporal_key_value_relations_counters[outer_key_item].keys())[list(temporal_key_value_relations_counters[outer_key_item].values()).index(max_value_item)]\n",
    "    #print(outer_key_item, max_key_item, max_value_item, round(float(max_value_item)/total_relation_number, 4))\n",
    "    for inner_key_item, inner_value_item in temporal_key_value_relations_counters[outer_key_item].items():\n",
    "        if inner_key_item != max_key_item:\n",
    "            outer_key_split = outer_key_item.split('\\t')\n",
    "            if inner_key_item != \"==\":\n",
    "                # In case the non-dominant relation found is of '<' or '>', denote it as automatically erroneous\n",
    "                automatically_produced_erroneous_patterns.add(str(outer_key_split[0]+\"\\t\"+inner_key_item+\"\\t\"+outer_key_split[1]))\n",
    "            else:\n",
    "                # In case the non-dominant relation found is '==', check if it among the ones that have been deemed as acceptable\n",
    "                exclude_from_erroneous = False\n",
    "                for equality_list in equality_cases_list:\n",
    "                    if outer_key_split[0] in equality_list and outer_key_split[1] in equality_list:\n",
    "                        exclude_from_erroneous = True\n",
    "                        break;\n",
    "                # If the current '==' variant was not deemed acceptable, according to the external information, consider it as erroneous\n",
    "                if not exclude_from_erroneous:\n",
    "                    automatically_produced_erroneous_patterns.add(str(outer_key_split[0]+\"\\t\"+inner_key_item+\"\\t\"+outer_key_split[1]))\n",
    "                        \n",
    "\n",
    "print(automatically_produced_erroneous_patterns)\n",
    "\n",
    "automatically_produced_erroneous_patterns = list(automatically_produced_erroneous_patterns)\n",
    "\n",
    "# Stores the indexes of the patterns found to be potentially inconsistent\n",
    "indexes_of_automatically_produced_erroneous_patterns = []\n",
    "\n",
    "for item_idx in range(len(automatically_produced_erroneous_patterns)):\n",
    "    indexes_of_automatically_produced_erroneous_patterns.append(temporal_relation_names_list.index(automatically_produced_erroneous_patterns[item_idx]))\n",
    "\n",
    "print(indexes_of_automatically_produced_erroneous_patterns)\n",
    "\n",
    "# Stores the subjects (with subject referring to s from the <s,p,o> triples), who have at least one pair of inonsistent temporal triples,\n",
    "# which is a pair of triples satisfying a temporal relationship variant found to be inconsistent.\n",
    "automatically_erroneous_subject_list = []\n",
    "for item_idx in range(len(s_and_temporal_true_index_list)):\n",
    "    if len(set(indexes_of_automatically_produced_erroneous_patterns).intersection(set(s_and_temporal_true_index_list[item_idx][1:]))) > 0:\n",
    "        automatically_erroneous_subject_list.append(s_and_temporal_true_index_list[item_idx][0])\n",
    "\n",
    "destination_file_name = \"semi-automatically_inconsistent_triples_file.txt\"\n",
    "automatically_erroneous_triple_file_name = HOME_FOLDER_PATH + OUTPUT_FOLDER_PATH + destination_file_name\n",
    "automatically_erroneous_triple_file_contents = []\n",
    "\n",
    "# Prints to file all the triples sharing the erroneous subject\n",
    "for item_idx in range(len(temporal_triple_strings_list)):\n",
    "    current_subject = temporal_triple_strings_list[item_idx].split('\\t')[0]\n",
    "    #print(temporal_triple_strings_list[item_idx], current_subject)\n",
    "    if current_subject in automatically_erroneous_subject_list:\n",
    "        #print(temporal_triple_strings_list[item_idx], current_subject, automatically_erroneous_subject_list[0])\n",
    "        automatically_erroneous_triple_file_contents.append(temporal_triple_strings_list[item_idx])\n",
    "\n",
    "automatically_erroneous_triples_file = open(automatically_erroneous_triple_file_name, \"w+\", encoding=\"utf8\")\n",
    "for list_idx in range(len(automatically_erroneous_triple_file_contents)):\n",
    "    automatically_erroneous_triples_file.write(automatically_erroneous_triple_file_contents[list_idx]+\"\\n\")\n",
    "automatically_erroneous_triples_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bb58845-53c6-4f41-8435-62b40b0f50df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<http://schema.org/startDate>', '<http://schema.org/endDate>'], ['<http://schema.org/endDate>', '<http://schema.org/startDate>']]\n",
      "{'<http://schema.org/deathDate>\\t==\\t<http://schema.org/birthDate>', '<http://schema.org/dateCreated>\\t==\\t<http://schema.org/dissolutionDate>', '<http://schema.org/dissolutionDate>\\t==\\t<http://schema.org/dateCreated>', '<http://schema.org/startDate>\\t>\\t<http://schema.org/endDate>', '<http://schema.org/birthDate>\\t>\\t<http://schema.org/deathDate>', '<http://schema.org/dissolutionDate>\\t<\\t<http://schema.org/dateCreated>', '<http://schema.org/dateCreated>\\t>\\t<http://schema.org/dissolutionDate>', '<http://schema.org/birthDate>\\t==\\t<http://schema.org/deathDate>', '<http://schema.org/deathDate>\\t<\\t<http://schema.org/birthDate>'}\n",
      "[14, 7, 16, 15, 5, 13, 11, 10, 6]\n",
      "<http://yago-knowledge.org/resource/Alexios_Palaiologos__u0028_despot_u0029_> 73779 days 00:00:00 73050 days, 0:00:00\n"
     ]
    }
   ],
   "source": [
    "# This applies all the tests: the automatic version that relies on internal information and then the two external information\n",
    "# methods; firstly the one that relaxes which temporal relation variant is the dominant, by also considering the equality, whenever\n",
    "# this is specified, and then by also creating the externally specified validity intervals for specific temporal relations.\n",
    "automatically_produced_erroneous_patterns = set()\n",
    "\n",
    "# The list with the cases where, for temporal relations with an inequality dominant variant, the equality is also acceptable\n",
    "equality_cases_list = acceptable_equality_cases()\n",
    "print(equality_cases_list)\n",
    "\n",
    "# Find the erroneous patterns and print to a file all the erroneous triples\n",
    "for outer_key_item in temporal_key_value_relations_counters.keys():\n",
    "    total_relation_number = sum(temporal_key_value_relations_counters[outer_key_item].values())  # Can be removed\n",
    "    # Get the value of the temporal relation item with the maximum value, which is considered the dominant variant\n",
    "    max_value_item = max(list(temporal_key_value_relations_counters[outer_key_item].values()))\n",
    "    # Get the '<', '>' or '==' symbol that corresponds to the variant of the temporal relation with the maximum value\n",
    "    max_key_item = list(temporal_key_value_relations_counters[outer_key_item].keys())[list(temporal_key_value_relations_counters[outer_key_item].values()).index(max_value_item)]\n",
    "    #print(outer_key_item, max_key_item, max_value_item, round(float(max_value_item)/total_relation_number, 4))\n",
    "    for inner_key_item, inner_value_item in temporal_key_value_relations_counters[outer_key_item].items():\n",
    "        if inner_key_item != max_key_item:\n",
    "            outer_key_split = outer_key_item.split('\\t')\n",
    "            if inner_key_item != \"==\":\n",
    "                # In case the non-dominant relation found is of '<' or '>', denote it as automatically erroneous\n",
    "                automatically_produced_erroneous_patterns.add(str(outer_key_split[0]+\"\\t\"+inner_key_item+\"\\t\"+outer_key_split[1]))\n",
    "            else:\n",
    "                # In case the non-dominant relation found is '==', check if it among the ones that have been deemed as acceptable\n",
    "                exclude_from_erroneous = False\n",
    "                for equality_list in equality_cases_list:\n",
    "                    if outer_key_split[0] in equality_list and outer_key_split[1] in equality_list:\n",
    "                        exclude_from_erroneous = True\n",
    "                        break;\n",
    "                # If the current '==' variant was not deemed acceptable, according to the external information, consider it as erroneous\n",
    "                if not exclude_from_erroneous:\n",
    "                    automatically_produced_erroneous_patterns.add(str(outer_key_split[0]+\"\\t\"+inner_key_item+\"\\t\"+outer_key_split[1]))\n",
    "                        \n",
    "\n",
    "print(automatically_produced_erroneous_patterns)\n",
    "\n",
    "automatically_produced_erroneous_patterns = list(automatically_produced_erroneous_patterns)\n",
    "\n",
    "# Stores the indexes of the patterns found to be potentially inconsistent\n",
    "indexes_of_automatically_produced_erroneous_patterns = []\n",
    "\n",
    "for item_idx in range(len(automatically_produced_erroneous_patterns)):\n",
    "    indexes_of_automatically_produced_erroneous_patterns.append(temporal_relation_names_list.index(automatically_produced_erroneous_patterns[item_idx]))\n",
    "\n",
    "print(indexes_of_automatically_produced_erroneous_patterns)\n",
    "\n",
    "# Stores the subjects (with subject referring to s from the <s,p,o> triples), who have at least one pair of inonsistent temporal triples,\n",
    "# which is a pair of triples satisfying a temporal relationship variant found to be inconsistent.\n",
    "automatically_erroneous_subject_list = []\n",
    "for item_idx in range(len(s_and_temporal_true_index_list)):\n",
    "    if len(set(indexes_of_automatically_produced_erroneous_patterns).intersection(set(s_and_temporal_true_index_list[item_idx][1:]))) > 0:\n",
    "        automatically_erroneous_subject_list.append(s_and_temporal_true_index_list[item_idx][0])\n",
    "\n",
    "# A list of lists where each internal list contains the pair of predicates that define an interval,\n",
    "# as well as the lower and upper limits of the possible values for said interval.\n",
    "# Its creation is based on a file and each line of said file contains two predicates,\n",
    "# an upper and a lower limit (from which one might be \"-\" if it is not applicable), separated by a tab.\n",
    "interval_limits_list = acceptable_interval_limits()\n",
    "\n",
    "# The key_subject is the subject from the <s,p,o> triples\n",
    "for key_subject, values_dict in temporal_intervals_dict.items():\n",
    "    # The values dict contains the temporal relations (in the form \"temporal_predicate_1\\ttemporal_predicate_2\")\n",
    "    # as keys and the corresponding intervals as values.\n",
    "    for key_tempor_rel, value_interv in values_dict.items():\n",
    "        # Split the \"temporal_predicate_1\\ttemporal_predicate_2\" temporal relation into [\"temporal_predicate_1\", \"temporal_predicate_2\"]\n",
    "        tempor_rel_split = key_tempor_rel.split(\"\\t\")\n",
    "        for curr_interval_limit in interval_limits_list:\n",
    "            #print(curr_interval_limit[:2])\n",
    "            # This should better NOT be a set\n",
    "            # If the predicates defining an interval match the currently examined triple's interval,\n",
    "            # then check whether the upper and/or lower bounds are violated\n",
    "            if set([tempor_rel_split[0], tempor_rel_split[1]]) == set(curr_interval_limit[:2]):\n",
    "                if curr_interval_limit[2] != \"-\":\n",
    "                    #print(value_interv.length, timedelta(days=int(curr_interval_limit[2])))\n",
    "                    if value_interv.length < timedelta(days=int(curr_interval_limit[2])):\n",
    "                        # Erroneous value, add to the list\n",
    "                        if key_subject not in automatically_erroneous_subject_list:\n",
    "                            automatically_erroneous_subject_list.append(key_subject)\n",
    "                            print(key_subject, value_interv.length, timedelta(days=int(curr_interval_limit[2])))\n",
    "                if curr_interval_limit[3] != \"-\":\n",
    "                    #print(value_interv.length, timedelta(days=int(curr_interval_limit[3])))\n",
    "                    if value_interv.length > timedelta(days=int(curr_interval_limit[3])):\n",
    "                        # Erroneous value, add to the list\n",
    "                        if key_subject not in automatically_erroneous_subject_list:\n",
    "                            automatically_erroneous_subject_list.append(key_subject)\n",
    "                            print(key_subject, value_interv.length, timedelta(days=int(curr_interval_limit[3])))\n",
    "\n",
    "destination_file_name = \"full_semi-automatically_inconsistent_triples_file.txt\"\n",
    "automatically_erroneous_triple_file_name = HOME_FOLDER_PATH + OUTPUT_FOLDER_PATH + destination_file_name\n",
    "automatically_erroneous_triple_file_contents = []\n",
    "\n",
    "# Prints to file all the triples sharing the erroneous subject\n",
    "for item_idx in range(len(temporal_triple_strings_list)):\n",
    "    current_subject = temporal_triple_strings_list[item_idx].split('\\t')[0]\n",
    "    #print(temporal_triple_strings_list[item_idx], current_subject)\n",
    "    if current_subject in automatically_erroneous_subject_list:\n",
    "        #print(temporal_triple_strings_list[item_idx], current_subject, automatically_erroneous_subject_list[0])\n",
    "        automatically_erroneous_triple_file_contents.append(temporal_triple_strings_list[item_idx])\n",
    "\n",
    "automatically_erroneous_triples_file = open(automatically_erroneous_triple_file_name, \"w+\", encoding=\"utf8\")\n",
    "for list_idx in range(len(automatically_erroneous_triple_file_contents)):\n",
    "    automatically_erroneous_triples_file.write(automatically_erroneous_triple_file_contents[list_idx]+\"\\n\")\n",
    "automatically_erroneous_triples_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612a13f-7703-4714-b0d8-38627e991c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
